{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Resolution Data Exploration Notebook\n",
    "\n",
    "This notebook explores the Yale University Library catalog data to better understand patterns and characteristics that will inform the entity resolution process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "import weaviate\n",
    "\n",
    "# Add parent directory to path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import utilities\n",
    "from src.utils import compute_vector_similarity, compute_levenshtein_similarity\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = Path('../config.yml')\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Set data directory\n",
    "data_dir = Path(config['dataset']['input_dir'])\n",
    "ground_truth_file = Path(config['dataset']['ground_truth_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available data files\n",
    "csv_files = list(data_dir.glob('*.csv'))\n",
    "print(f\"Found {len(csv_files)} CSV files in {data_dir}\")\n",
    "\n",
    "# Load ground truth data if available\n",
    "if ground_truth_file.exists():\n",
    "    ground_truth_df = pd.read_csv(ground_truth_file)\n",
    "    print(f\"Ground truth data: {len(ground_truth_df)} labeled pairs\")\n",
    "    print(f\"Positive examples: {ground_truth_df['match'].sum()} ({ground_truth_df['match'].mean()*100:.1f}%)\")\n",
    "    print(f\"Negative examples: {len(ground_truth_df) - ground_truth_df['match'].sum()} ({(1-ground_truth_df['match'].mean())*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"Ground truth file not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial Data Inspection\n",
    "\n",
    "Let's load a sample of the data to understand its structure and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample of CSV files (limited to first 5 for exploration)\n",
    "sample_files = csv_files[:5]\n",
    "dfs = []\n",
    "\n",
    "for file in sample_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        df['source_file'] = file.name\n",
    "        dfs.append(df)\n",
    "        print(f\"Loaded {file.name}: {len(df)} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file.name}: {e}\")\n",
    "\n",
    "# Combine into a single dataframe for exploration\n",
    "if dfs:\n",
    "    sample_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"\\nCombined sample dataset: {len(sample_df)} records\")\n",
    "    \n",
    "    # Display the first few rows\n",
    "    display(sample_df.head())\n",
    "    \n",
    "    # Display column information\n",
    "    print(\"\\nColumn information:\")\n",
    "    for col in sample_df.columns:\n",
    "        if col != 'source_file':\n",
    "            na_count = sample_df[col].isna().sum()\n",
    "            na_percent = na_count / len(sample_df) * 100\n",
    "            print(f\"{col}: {na_count} null values ({na_percent:.1f}%)\")\n",
    "else:\n",
    "    print(\"No data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Person Entity Analysis\n",
    "\n",
    "Let's analyze the 'person' field to understand name patterns and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sample_df' in locals() and 'person' in sample_df.columns:\n",
    "    # Count unique person names\n",
    "    unique_persons = sample_df['person'].unique()\n",
    "    print(f\"Total unique person names: {len(unique_persons)}\")\n",
    "    \n",
    "    # Frequency distribution\n",
    "    person_counts = sample_df['person'].value_counts()\n",
    "    \n",
    "    print(f\"\\nTop 10 most frequent person names:\")\n",
    "    display(person_counts.head(10))\n",
    "    \n",
    "    # Plot frequency distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    person_counts.head(20).plot(kind='bar')\n",
    "    plt.title('Top 20 Most Frequent Person Names')\n",
    "    plt.xlabel('Person Name')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze name patterns\n",
    "    print(\"\\nName pattern analysis:\")\n",
    "    \n",
    "    # Check for names with birth/death years\n",
    "    birth_death_pattern = re.compile(r',\\s*(\\d{4})-(\\d{4}|\\?)')\n",
    "    names_with_years = [name for name in unique_persons if birth_death_pattern.search(str(name))]\n",
    "    \n",
    "    print(f\"Names with birth/death years: {len(names_with_years)} ({len(names_with_years)/len(unique_persons)*100:.1f}%)\")\n",
    "    \n",
    "    if names_with_years:\n",
    "        print(\"\\nSample names with birth/death years:\")\n",
    "        for name in names_with_years[:5]:\n",
    "            print(f\"  - {name}\")\n",
    "    \n",
    "    # Check for names with suffixes (Jr., Sr., III, etc.)\n",
    "    suffix_pattern = re.compile(r',\\s+(Jr\\.|Sr\\.|I{2,}|IV|V)\\b')\n",
    "    names_with_suffix = [name for name in unique_persons if suffix_pattern.search(str(name))]\n",
    "    \n",
    "    print(f\"\\nNames with suffixes (Jr., Sr., etc.): {len(names_with_suffix)} ({len(names_with_suffix)/len(unique_persons)*100:.1f}%)\")\n",
    "    \n",
    "    if names_with_suffix:\n",
    "        print(\"\\nSample names with suffixes:\")\n",
    "        for name in names_with_suffix[:5]:\n",
    "            print(f\"  - {name}\")\n",
    "    \n",
    "    # Distribution of name lengths\n",
    "    name_lengths = [len(str(name)) for name in unique_persons]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(name_lengths, bins=30, kde=True)\n",
    "    plt.title('Distribution of Person Name Lengths')\n",
    "    plt.xlabel('Name Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze structure of names (Last, First vs. First Last)\n",
    "    comma_names = [name for name in unique_persons if ',' in str(name)]\n",
    "    print(f\"\\nNames in 'Last, First' format: {len(comma_names)} ({len(comma_names)/len(unique_persons)*100:.1f}%)\")\n",
    "    print(f\"Names in other formats: {len(unique_persons) - len(comma_names)} ({(len(unique_persons) - len(comma_names))/len(unique_persons)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"Person data not available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Role and Title Analysis\n",
    "\n",
    "Let's analyze the roles and titles associated with persons to understand the context of their contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sample_df' in locals():\n",
    "    # Analyze roles distribution\n",
    "    if 'roles' in sample_df.columns:\n",
    "        role_counts = sample_df['roles'].value_counts()\n",
    "        \n",
    "        print(f\"Unique roles: {len(role_counts)}\")\n",
    "        print(\"\\nTop 10 most common roles:\")\n",
    "        display(role_counts.head(10))\n",
    "        \n",
    "        # Plot role distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        role_counts.head(10).plot(kind='bar')\n",
    "        plt.title('Top 10 Most Common Roles')\n",
    "        plt.xlabel('Role')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Roles data not available for analysis\")\n",
    "    \n",
    "    # Analyze title characteristics\n",
    "    if 'title' in sample_df.columns:\n",
    "        # Title length distribution\n",
    "        title_lengths = sample_df['title'].str.len()\n",
    "        \n",
    "        print(f\"\\nTitle length statistics:\")\n",
    "        print(f\"Min: {title_lengths.min()}\")\n",
    "        print(f\"Max: {title_lengths.max()}\")\n",
    "        print(f\"Mean: {title_lengths.mean():.1f}\")\n",
    "        print(f\"Median: {title_lengths.median():.1f}\")\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(title_lengths, bins=30, kde=True)\n",
    "        plt.title('Distribution of Title Lengths')\n",
    "        plt.xlabel('Title Length (characters)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Title data not available for analysis\")\n",
    "else:\n",
    "    print(\"Sample data not available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Temporal Analysis\n",
    "\n",
    "Let's analyze temporal aspects of the data to understand the distribution of publication dates and person lifespans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sample_df' in locals():\n",
    "    # Extract years from provision field\n",
    "    if 'provision' in sample_df.columns:\n",
    "        # Extract years using regex\n",
    "        year_pattern = re.compile(r'\\b(1[5-9]\\d{2}|20\\d{2})\\b')\n",
    "        \n",
    "        years = []\n",
    "        for provision in sample_df['provision'].dropna():\n",
    "            matches = year_pattern.findall(str(provision))\n",
    "            if matches:\n",
    "                years.append(int(matches[0]))  # Take the first year found\n",
    "        \n",
    "        if years:\n",
    "            print(f\"Extracted {len(years)} publication years\")\n",
    "            \n",
    "            # Year distribution\n",
    "            year_counts = pd.Series(years).value_counts().sort_index()\n",
    "            \n",
    "            print(f\"\\nPublication year range: {min(years)} - {max(years)}\")\n",
    "            \n",
    "            plt.figure(figsize=(15, 6))\n",
    "            year_counts.plot(kind='line')\n",
    "            plt.title('Publication Year Distribution')\n",
    "            plt.xlabel('Year')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Group by century\n",
    "            centuries = []\n",
    "            for year in years:\n",
    "                century = (year // 100) + 1\n",
    "                centuries.append(f\"{century}th century\")\n",
    "            \n",
    "            century_counts = pd.Series(centuries).value_counts().sort_index()\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            century_counts.plot(kind='bar')\n",
    "            plt.title('Distribution by Century')\n",
    "            plt.xlabel('Century')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No publication years extracted\")\n",
    "    else:\n",
    "        print(\"Provision data not available for temporal analysis\")\n",
    "    \n",
    "    # Extract birth/death years from person names\n",
    "    if 'person' in sample_df.columns:\n",
    "        birth_death_pattern = re.compile(r',\\s*(\\d{4})-(\\d{4}|\\?)')\n",
    "        \n",
    "        birth_years = []\n",
    "        death_years = []\n",
    "        lifespans = []\n",
    "        \n",
    "        for name in sample_df['person'].dropna():\n",
    "            match = birth_death_pattern.search(str(name))\n",
    "            if match:\n",
    "                birth_year = int(match.group(1))\n",
    "                birth_years.append(birth_year)\n",
    "                \n",
    "                death_year_str = match.group(2)\n",
    "                if death_year_str != '?':\n",
    "                    death_year = int(death_year_str)\n",
    "                    death_years.append(death_year)\n",
    "                    lifespans.append(death_year - birth_year)\n",
    "        \n",
    "        if birth_years:\n",
    "            print(f\"\\n\\nExtracted {len(birth_years)} birth years and {len(death_years)} death years\")\n",
    "            \n",
    "            # Birth year distribution\n",
    "            plt.figure(figsize=(15, 6))\n",
    "            sns.histplot(birth_years, bins=30, kde=True)\n",
    "            plt.title('Birth Year Distribution')\n",
    "            plt.xlabel('Birth Year')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Lifespan distribution\n",
    "            if lifespans:\n",
    "                print(f\"\\nLifespan statistics:\")\n",
    "                print(f\"Min: {min(lifespans)} years\")\n",
    "                print(f\"Max: {max(lifespans)} years\")\n",
    "                print(f\"Mean: {np.mean(lifespans):.1f} years\")\n",
    "                print(f\"Median: {np.median(lifespans):.1f} years\")\n",
    "                \n",
    "                plt.figure(figsize=(10, 6))\n",
    "                sns.histplot(lifespans, bins=20, kde=True)\n",
    "                plt.title('Lifespan Distribution')\n",
    "                plt.xlabel('Lifespan (years)')\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.grid(True)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        else:\n",
    "            print(\"No birth/death years extracted\")\n",
    "else:\n",
    "    print(\"Sample data not available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Subject Analysis\n",
    "\n",
    "Let's analyze the subjects associated with persons to understand the distribution of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sample_df' in locals() and 'subjects' in sample_df.columns:\n",
    "    # Count null values\n",
    "    null_count = sample_df['subjects'].isna().sum()\n",
    "    print(f\"Subjects with null values: {null_count} ({null_count/len(sample_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Process non-null subjects\n",
    "    non_null_subjects = sample_df['subjects'].dropna()\n",
    "    \n",
    "    # Split subjects by delimiter (assuming semicolon or comma)\n",
    "    all_subjects = []\n",
    "    for subject_str in non_null_subjects:\n",
    "        # Try different delimiters\n",
    "        if ';' in subject_str:\n",
    "            subjects = [s.strip() for s in subject_str.split(';')]\n",
    "        elif ',' in subject_str:\n",
    "            subjects = [s.strip() for s in subject_str.split(',')]\n",
    "        else:\n",
    "            subjects = [subject_str.strip()]\n",
    "        all_subjects.extend(subjects)\n",
    "    \n",
    "    # Count subject frequency\n",
    "    subject_counts = Counter(all_subjects)\n",
    "    \n",
    "    print(f\"\\nTotal unique subjects: {len(subject_counts)}\")\n",
    "    print(f\"Total subject mentions: {sum(subject_counts.values())}\")\n",
    "    \n",
    "    # Most common subjects\n",
    "    print(\"\\nTop 20 most common subjects:\")\n",
    "    for subject, count in subject_counts.most_common(20):\n",
    "        print(f\"  - {subject}: {count}\")\n",
    "    \n",
    "    # Plot subject distribution\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_subjects = dict(subject_counts.most_common(15))\n",
    "    plt.bar(top_subjects.keys(), top_subjects.values())\n",
    "    plt.title('Top 15 Most Common Subjects')\n",
    "    plt.xlabel('Subject')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Subject length distribution\n",
    "    subject_lengths = [len(subject) for subject in all_subjects]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(subject_lengths, bins=30, kde=True)\n",
    "    plt.title('Distribution of Subject Lengths')\n",
    "    plt.xlabel('Subject Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Subject data not available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ground Truth Analysis\n",
    "\n",
    "Let's analyze the ground truth data to understand the characteristics of matching and non-matching pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ground_truth_df' in locals():\n",
    "    print(f\"Analyzing {len(ground_truth_df)} labeled pairs\")\n",
    "    \n",
    "    # Split into matches and non-matches\n",
    "    matches_df = ground_truth_df[ground_truth_df['match'] == True]\n",
    "    non_matches_df = ground_truth_df[ground_truth_df['match'] == False]\n",
    "    \n",
    "    print(f\"Matches: {len(matches_df)} ({len(matches_df)/len(ground_truth_df)*100:.1f}%)\")\n",
    "    print(f\"Non-matches: {len(non_matches_df)} ({len(non_matches_df)/len(ground_truth_df)*100:.1f}%)\")\n",
    "    \n",
    "    # If we have access to the sample data, try to find some example pairs\n",
    "    if 'sample_df' in locals():\n",
    "        print(\"\\nSample matching pairs:\")\n",
    "        match_count = 0\n",
    "        \n",
    "        for _, row in matches_df.head(5).iterrows():\n",
    "            left_id = row['left']\n",
    "            right_id = row['right']\n",
    "            \n",
    "            left_record = sample_df[sample_df['personId'] == left_id]\n",
    "            right_record = sample_df[sample_df['personId'] == right_id]\n",
    "            \n",
    "            if not left_record.empty and not right_record.empty:\n",
    "                match_count += 1\n",
    "                print(f\"\\nExample {match_count}:\")\n",
    "                print(f\"Left:  {left_record['person'].values[0]} - {left_record['title'].values[0][:50]}...\")\n",
    "                print(f\"Right: {right_record['person'].values[0]} - {right_record['title'].values[0][:50]}...\")\n",
    "        \n",
    "        if match_count == 0:\n",
    "            print(\"No matching examples found in the sample data\")\n",
    "        \n",
    "        print(\"\\nSample non-matching pairs:\")\n",
    "        non_match_count = 0\n",
    "        \n",
    "        for _, row in non_matches_df.head(5).iterrows():\n",
    "            left_id = row['left']\n",
    "            right_id = row['right']\n",
    "            \n",
    "            left_record = sample_df[sample_df['personId'] == left_id]\n",
    "            right_record = sample_df[sample_df['personId'] == right_id]\n",
    "            \n",
    "            if not left_record.empty and not right_record.empty:\n",
    "                non_match_count += 1\n",
    "                print(f\"\\nExample {non_match_count}:\")\n",
    "                print(f\"Left:  {left_record['person'].values[0]} - {left_record['title'].values[0][:50]}...\")\n",
    "                print(f\"Right: {right_record['person'].values[0]} - {right_record['title'].values[0][:50]}...\")\n",
    "        \n",
    "        if non_match_count == 0:\n",
    "            print(\"No non-matching examples found in the sample data\")\n",
    "else:\n",
    "    print(\"Ground truth data not available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Name Similarity Analysis\n",
    "\n",
    "Let's analyze the similarity between person names to understand potential matching challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sample_df' in locals() and 'person' in sample_df.columns:\n",
    "    # Sample a subset of unique person names\n",
    "    unique_persons = sample_df['person'].unique()\n",
    "    \n",
    "    # Limit to 1000 names for performance\n",
    "    if len(unique_persons) > 1000:\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        unique_persons = np.random.choice(unique_persons, 1000, replace=False)\n",
    "    \n",
    "    print(f\"Analyzing similarities among {len(unique_persons)} unique person names\")\n",
    "    \n",
    "    # Compute Levenshtein similarities for a sample of pairs\n",
    "    similarities = []\n",
    "    sample_size = min(10000, len(unique_persons) * (len(unique_persons) - 1) // 2)\n",
    "    \n",
    "    for _ in range(sample_size):\n",
    "        # Sample two different names\n",
    "        idx1, idx2 = np.random.choice(len(unique_persons), 2, replace=False)\n",
    "        name1 = str(unique_persons[idx1])\n",
    "        name2 = str(unique_persons[idx2])\n",
    "        \n",
    "        # Compute similarity\n",
    "        similarity = compute_levenshtein_similarity(name1, name2)\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    # Analyze similarity distribution\n",
    "    print(f\"\\nLevenshtein similarity statistics:\")\n",
    "    print(f\"Min: {min(similarities):.4f}\")\n",
    "    print(f\"Max: {max(similarities):.4f}\")\n",
    "    print(f\"Mean: {np.mean(similarities):.4f}\")\n",
    "    print(f\"Median: {np.median(similarities):.4f}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(similarities, bins=50, kde=True)\n",
    "    plt.title('Distribution of Levenshtein Similarities Between Person Names')\n",
    "    plt.xlabel('Similarity (0-1)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.axvline(x=0.7, color='r', linestyle='--', label='Typical similarity threshold (0.7)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find examples of highly similar but different names\n",
    "    high_similarity_pairs = []\n",
    "    similarity_threshold = 0.8\n",
    "    \n",
    "    for _ in range(1000):  # Sample more pairs to find high similarity examples\n",
    "        idx1, idx2 = np.random.choice(len(unique_persons), 2, replace=False)\n",
    "        name1 = str(unique_persons[idx1])\n",
    "        name2 = str(unique_persons[idx2])\n",
    "        \n",
    "        # Skip exact matches\n",
    "        if name1 == name2:\n",
    "            continue\n",
    "        \n",
    "        similarity = compute_levenshtein_similarity(name1, name2)\n",
    "        if similarity >= similarity_threshold:\n",
    "            high_similarity_pairs.append((name1, name2, similarity))\n",
    "    \n",
    "    # Display high similarity pairs\n",
    "    if high_similarity_pairs:\n",
    "        print(f\"\\n{len(high_similarity_pairs)} pairs with similarity >= {similarity_threshold}:\")\n",
    "        for name1, name2, similarity in sorted(high_similarity_pairs, key=lambda x: x[2], reverse=True)[:10]:\n",
    "            print(f\"  - '{name1}' / '{name2}': {similarity:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\nNo pairs found with similarity >= {similarity_threshold}\")\n",
    "else:\n",
    "    print(\"Person data not available for similarity analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Vector Embedding Visualization\n",
    "\n",
    "Let's generate and visualize some example embeddings to understand how they might differentiate entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This requires access to OpenAI API, so it may not run in all environments\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Optional: Load OpenAI API key from environment or config\n",
    "api_key = os.environ.get(config['openai']['api_key_env'])\n",
    "\n",
    "if api_key:\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    if 'sample_df' in locals() and 'person' in sample_df.columns:\n",
    "        # Sample a small subset of person names for visualization\n",
    "        sample_names = sample_df['person'].dropna().unique()\n",
    "        if len(sample_names) > 30:\n",
    "            np.random.seed(42)  # For reproducibility\n",
    "            sample_names = np.random.choice(sample_names, 30, replace=False)\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(sample_names)} person names\")\n",
    "        \n",
    "        try:\n",
    "            # Generate embeddings\n",
    "            response = client.embeddings.create(\n",
    "                model=config['openai']['embedding_model'],\n",
    "                input=[str(name) for name in sample_names]\n",
    "            )\n",
    "            \n",
    "            # Extract embeddings\n",
    "            embeddings = [data.embedding for data in response.data]\n",
    "            \n",
    "            # Convert to numpy array\n",
    "            embeddings_array = np.array(embeddings)\n",
    "            \n",
    "            print(f\"Embedding shape: {embeddings_array.shape}\")\n",
    "            \n",
    "            # Reduce dimensionality for visualization\n",
    "            tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "            embeddings_2d = tsne.fit_transform(embeddings_array)\n",
    "            \n",
    "            # Create dataframe for plotting\n",
    "            viz_df = pd.DataFrame({\n",
    "                'x': embeddings_2d[:, 0],\n",
    "                'y': embeddings_2d[:, 1],\n",
    "                'name': [str(name) for name in sample_names]\n",
    "            })\n",
    "            \n",
    "            # Plot embeddings\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            sns.scatterplot(data=viz_df, x='x', y='y', s=100)\n",
    "            \n",
    "            # Add name labels\n",
    "            for _, row in viz_df.iterrows():\n",
    "                plt.text(row['x'], row['y'], row['name'].split(',')[0], fontsize=9)\n",
    "            \n",
    "            plt.title('t-SNE Visualization of Person Name Embeddings')\n",
    "            plt.xlabel('t-SNE Dimension 1')\n",
    "            plt.ylabel('t-SNE Dimension 2')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Compute pairwise cosine similarities\n",
    "            similarities = np.zeros((len(embeddings), len(embeddings)))\n",
    "            for i in range(len(embeddings)):\n",
    "                for j in range(len(embeddings)):\n",
    "                    similarities[i, j] = compute_vector_similarity(\n",
    "                        embeddings_array[i], embeddings_array[j], metric='cosine'\n",
    "                    )\n",
    "            \n",
    "            # Create similarity heatmap\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            sns.heatmap(similarities, annot=False, cmap='viridis',\n",
    "                       xticklabels=[name.split(',')[0] for name in sample_names],\n",
    "                       yticklabels=[name.split(',')[0] for name in sample_names])\n",
    "            plt.title('Cosine Similarity Heatmap of Person Name Embeddings')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Find most similar pairs\n",
    "            similar_pairs = []\n",
    "            for i in range(len(embeddings)):\n",
    "                for j in range(i+1, len(embeddings)):\n",
    "                    similarity = similarities[i, j]\n",
    "                    similar_pairs.append((sample_names[i], sample_names[j], similarity))\n",
    "            \n",
    "            # Sort by similarity\n",
    "            similar_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "            \n",
    "            print(\"\\nMost similar name pairs (by embedding):\")\n",
    "            for name1, name2, similarity in similar_pairs[:10]:\n",
    "                print(f\"  - '{name1}' / '{name2}': {similarity:.4f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embeddings: {e}\")\n",
    "else:\n",
    "    print(\"OpenAI API key not available for embedding visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Entity Resolution Challenges\n",
    "\n",
    "Based on our exploration, here are the key challenges for entity resolution in this dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Name Variation Challenges\n",
    "\n",
    "1. **Name Format Variations**:\n",
    "   - Last, First vs. First Last\n",
    "   - Inclusion/exclusion of middle names or initials\n",
    "   - Variations in suffixes (Jr., Sr., III)\n",
    "\n",
    "2. **Name Completeness**:\n",
    "   - Some entries have birth/death years, others don't\n",
    "   - Abbreviated vs. full names\n",
    "   - Inclusion/exclusion of titles (Dr., Prof.)\n",
    "\n",
    "3. **Common Names**:\n",
    "   - Many common names that could refer to different individuals\n",
    "   - Need to use contextual information from other fields\n",
    "\n",
    "### 9.2 Temporal Challenges\n",
    "\n",
    "1. **Publication Date vs. Author Lifetime**:\n",
    "   - Publications may occur long after an author's death\n",
    "   - Historical figures may have works published centuries later\n",
    "\n",
    "2. **Missing Temporal Information**:\n",
    "   - Many records lack birth/death years\n",
    "   - Publication dates may be uncertain or missing\n",
    "\n",
    "### 9.3 Role and Context Challenges\n",
    "\n",
    "1. **Multiple Roles**:\n",
    "   - Same person may appear with different roles across records\n",
    "   - Need to distinguish between primary and secondary roles\n",
    "\n",
    "2. **Subject Matter Diversity**:\n",
    "   - Same person may be associated with diverse subjects\n",
    "   - Need to account for multidisciplinary individuals\n",
    "\n",
    "### 9.4 Data Quality Challenges\n",
    "\n",
    "1. **Missing Values**:\n",
    "   - Significant proportion of records with null values in some fields\n",
    "   - Need robust imputation strategies\n",
    "\n",
    "2. **Inconsistent Formats**:\n",
    "   - Variations in how the same information is recorded\n",
    "   - Need to normalize data before comparison\n",
    "\n",
    "### 9.5 Multilingual Challenges\n",
    "\n",
    "1. **Name Transliteration**:\n",
    "   - Same person may have differently transliterated names\n",
    "   - Names from non-Latin scripts may have variant spellings\n",
    "\n",
    "2. **Title/Subject Translation**:\n",
    "   - Same work may have titles in different languages\n",
    "   - Subject terms may be in multiple languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions and Recommendations\n",
    "\n",
    "Based on our exploration, here are key recommendations for the entity resolution pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Feature Engineering Recommendations\n",
    "\n",
    "1. **Name-Based Features**:\n",
    "   - Create specialized features for name comparison (normalized Levenshtein)\n",
    "   - Implement logic to handle birth/death years as strong signals\n",
    "   - Develop features for name parts (first name, last name, middle initial)\n",
    "\n",
    "2. **Contextual Features**:\n",
    "   - Use title and subject similarity as contextual signals\n",
    "   - Develop temporal overlap features that account for posthumous publications\n",
    "   - Create interaction features between name and context similarities\n",
    "\n",
    "3. **Vector-Based Features**:\n",
    "   - Use field-specific embeddings for specialized comparison\n",
    "   - Create composite embeddings that combine multiple fields\n",
    "   - Implement harmonic means of similarities to handle missing values\n",
    "\n",
    "### 10.2 Architecture Recommendations\n",
    "\n",
    "1. **Robust Imputation**:\n",
    "   - Implement vector-based hot deck imputation for missing fields\n",
    "   - Create specialized imputation strategies for temporal data\n",
    "\n",
    "2. **Similarity Thresholds**:\n",
    "   - Use adaptive thresholds based on name uniqueness\n",
    "   - Implement higher thresholds for common names\n",
    "\n",
    "3. **Classification Strategy**:\n",
    "   - Implement a two-stage approach: blocking followed by detailed comparison\n",
    "   - Use embeddings for efficient blocking\n",
    "   - Prioritize precision over recall for ambiguous cases\n",
    "\n",
    "4. **Result Validation**:\n",
    "   - Implement logical constraints to ensure consistency\n",
    "   - Flag potentially problematic matches for human review\n",
    "   - Create a feedback mechanism to learn from corrections\n",
    "\n",
    "### 10.3 Implementation Considerations\n",
    "\n",
    "1. **Scalability**:\n",
    "   - Optimize batch size for API rate limits\n",
    "   - Implement efficient vector indexing with Weaviate\n",
    "   - Use parallel processing for feature extraction\n",
    "\n",
    "2. **Evaluation Strategy**:\n",
    "   - Develop specialized metrics for library catalog contexts\n",
    "   - Create visualization tools for cluster analysis\n",
    "   - Implement detailed error analysis capabilities\n",
    "\n",
    "3. **Extensibility**:\n",
    "   - Design for multilingual support from the beginning\n",
    "   - Create configurable feature sets for different collection types\n",
    "   - Implement modular architecture for easy component updates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
