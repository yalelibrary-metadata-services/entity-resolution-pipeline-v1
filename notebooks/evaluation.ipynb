{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Resolution Evaluation Notebook\n",
    "\n",
    "This notebook evaluates the results of the entity resolution pipeline, analyzing the performance of various stages and the final clustering outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import jsonlines\n",
    "\n",
    "# Add parent directory to path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import pipeline modules as needed\n",
    "from src.utils import compute_vector_similarity, compute_levenshtein_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = Path('../config.yml')\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Set paths\n",
    "output_dir = Path(config['general']['output_dir'])\n",
    "analysis_dir = output_dir / 'analysis'\n",
    "detailed_dir = output_dir / 'detailed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Summary Report Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load summary report\n",
    "summary_path = output_dir / 'summary_report.json'\n",
    "if summary_path.exists():\n",
    "    with open(summary_path, 'r') as f:\n",
    "        summary = json.load(f)\n",
    "    \n",
    "    # Display summary information\n",
    "    print(f\"Pipeline executed on: {summary['timestamp']}\\n\")\n",
    "    print(f\"Mode: {summary['config']['mode']}\")\n",
    "    print(f\"Embedding model: {summary['config']['openai_model']}\\n\")\n",
    "    \n",
    "    print(\"=== Preprocessing ===\")\n",
    "    print(f\"Unique strings: {summary['preprocessing']['unique_strings']}\")\n",
    "    print(f\"Records: {summary['preprocessing']['records']}\")\n",
    "    print(f\"Person IDs: {summary['preprocessing']['person_ids']}\\n\")\n",
    "    \n",
    "    print(\"=== Classification and Clustering ===\")\n",
    "    print(f\"Matches: {summary['classification']['matches']}\")\n",
    "    print(f\"Match threshold: {summary['classification']['threshold']}\")\n",
    "    print(f\"Clusters: {summary['clustering']['clusters']}\")\n",
    "    print(f\"Clustering algorithm: {summary['clustering']['algorithm']}\\n\")\n",
    "    \n",
    "    # Calculate derived metrics\n",
    "    match_rate = summary['classification']['matches'] / summary['preprocessing']['person_ids']\n",
    "    avg_cluster_size = summary['preprocessing']['person_ids'] / summary['clustering']['clusters'] \\\n",
    "        if summary['clustering']['clusters'] > 0 else 0\n",
    "    \n",
    "    print(\"=== Derived Metrics ===\")\n",
    "    print(f\"Match rate: {match_rate:.4f} matches per person\")\n",
    "    print(f\"Average cluster size: {avg_cluster_size:.2f} persons per cluster\")\n",
    "else:\n",
    "    print(\"Summary report not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessing statistics\n",
    "preprocessing_stats_path = analysis_dir / 'preprocessing_stats.json'\n",
    "if preprocessing_stats_path.exists():\n",
    "    with open(preprocessing_stats_path, 'r') as f:\n",
    "        preprocessing_stats = json.load(f)\n",
    "    \n",
    "    # Display field distribution\n",
    "    field_counts = {field: data['unique_strings'] for field, data in preprocessing_stats['fields'].items()}\n",
    "    field_df = pd.DataFrame({\n",
    "        'Field': field_counts.keys(),\n",
    "        'Unique Strings': field_counts.values()\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=field_df, x='Field', y='Unique Strings')\n",
    "    plt.title('Unique Strings by Field')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display frequency distribution\n",
    "    print(\"\\n=== String Frequency Statistics ===\")\n",
    "    print(f\"Min frequency: {preprocessing_stats['frequency_distribution']['min']}\")\n",
    "    print(f\"Max frequency: {preprocessing_stats['frequency_distribution']['max']}\")\n",
    "    print(f\"Mean frequency: {preprocessing_stats['frequency_distribution']['mean']:.2f}\")\n",
    "    print(f\"Median frequency: {preprocessing_stats['frequency_distribution']['median']:.2f}\")\n",
    "    \n",
    "    # Plot frequency distribution (first 20 values)\n",
    "    freq_dist = preprocessing_stats['frequency_distribution']['counts']\n",
    "    freqs = [int(k) for k in freq_dist.keys()][:20]\n",
    "    counts = [freq_dist[str(f)] for f in freqs]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(freqs, counts)\n",
    "    plt.title('String Frequency Distribution')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Count')\n",
    "    plt.yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Preprocessing statistics not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classification metrics\n",
    "classification_metrics_path = detailed_dir / 'classification_metrics.json'\n",
    "if classification_metrics_path.exists():\n",
    "    with open(classification_metrics_path, 'r') as f:\n",
    "        classification_metrics = json.load(f)\n",
    "    \n",
    "    # Display training metrics\n",
    "    if 'test_metrics' in classification_metrics:\n",
    "        print(\"=== Test Set Metrics ===\")\n",
    "        test_metrics = classification_metrics['test_metrics']\n",
    "        print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {test_metrics['f1']:.4f}\\n\")\n",
    "    \n",
    "    # Display feature importance if available\n",
    "    if 'feature_importance' in classification_metrics:\n",
    "        feature_importance = classification_metrics['feature_importance']\n",
    "        feature_df = pd.DataFrame({\n",
    "            'Feature': feature_importance.keys(),\n",
    "            'Importance': feature_importance.values()\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        print(\"=== Feature Importance ===\")\n",
    "        display(feature_df)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(data=feature_df, x='Importance', y='Feature')\n",
    "        plt.title('Feature Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Classification metrics not found\")\n",
    "    \n",
    "# Load match pairs\n",
    "match_pairs_path = detailed_dir / 'match_pairs.csv'\n",
    "if match_pairs_path.exists():\n",
    "    match_pairs_df = pd.read_csv(match_pairs_path)\n",
    "    \n",
    "    print(f\"\\n=== Match Pairs Analysis ===\")\n",
    "    print(f\"Total match pairs: {len(match_pairs_df)}\")\n",
    "    \n",
    "    # Analyze confidence distribution\n",
    "    print(f\"\\nConfidence Distribution:\")\n",
    "    print(f\"Min: {match_pairs_df['confidence'].min():.4f}\")\n",
    "    print(f\"Max: {match_pairs_df['confidence'].max():.4f}\")\n",
    "    print(f\"Mean: {match_pairs_df['confidence'].mean():.4f}\")\n",
    "    print(f\"Median: {match_pairs_df['confidence'].median():.4f}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(match_pairs_df['confidence'], bins=20, alpha=0.7)\n",
    "    plt.title('Match Confidence Distribution')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Count')\n",
    "    plt.axvline(config['classification']['match_threshold'], color='red', linestyle='--', \n",
    "                label=f\"Threshold: {config['classification']['match_threshold']}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Match pairs not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clustering statistics\n",
    "clustering_stats_path = analysis_dir / 'clustering_stats.json'\n",
    "if clustering_stats_path.exists():\n",
    "    with open(clustering_stats_path, 'r') as f:\n",
    "        clustering_stats = json.load(f)\n",
    "    \n",
    "    print(\"=== Clustering Statistics ===\")\n",
    "    print(f\"Total clusters: {clustering_stats['total_clusters']}\")\n",
    "    print(f\"Total entities: {clustering_stats['total_entities']}\")\n",
    "    \n",
    "    # Cluster size distribution\n",
    "    print(f\"\\nCluster Size Distribution:\")\n",
    "    print(f\"Min size: {clustering_stats['cluster_size_distribution']['min']}\")\n",
    "    print(f\"Max size: {clustering_stats['cluster_size_distribution']['max']}\")\n",
    "    print(f\"Mean size: {clustering_stats['cluster_size_distribution']['mean']:.2f}\")\n",
    "    print(f\"Median size: {clustering_stats['cluster_size_distribution']['median']:.2f}\")\n",
    "    \n",
    "    # Plot cluster size histogram\n",
    "    bins = clustering_stats['cluster_size_histogram']['bins']\n",
    "    counts = clustering_stats['cluster_size_histogram']['counts']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(bins[:-1], counts, width=(bins[1]-bins[0]))\n",
    "    plt.title('Cluster Size Distribution')\n",
    "    plt.xlabel('Cluster Size')\n",
    "    plt.ylabel('Count')\n",
    "    plt.yscale('log')  # Log scale for better visualization\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Clustering statistics not found\")\n",
    "    \n",
    "# Load cluster assignments\n",
    "clusters_path = detailed_dir / 'clusters.csv'\n",
    "if clusters_path.exists():\n",
    "    clusters_df = pd.read_csv(clusters_path)\n",
    "    \n",
    "    # Analyze cluster distribution\n",
    "    cluster_sizes = clusters_df.groupby('cluster_id')['entity_id'].count().reset_index()\n",
    "    cluster_sizes = cluster_sizes.rename(columns={'entity_id': 'size'})\n",
    "    \n",
    "    # Top 10 largest clusters\n",
    "    top_clusters = cluster_sizes.sort_values('size', ascending=False).head(10)\n",
    "    \n",
    "    print(\"\\n=== Top 10 Largest Clusters ===\")\n",
    "    display(top_clusters)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=top_clusters, x='cluster_id', y='size')\n",
    "    plt.title('Top 10 Largest Clusters')\n",
    "    plt.xlabel('Cluster ID')\n",
    "    plt.ylabel('Size')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cluster assignments not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entity Resolution Quality Analysis\n",
    "\n",
    "If ground truth data is available, we can evaluate the quality of entity resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth data if available\n",
    "ground_truth_path = Path(config['dataset']['ground_truth_file'])\n",
    "if ground_truth_path.exists():\n",
    "    ground_truth_df = pd.read_csv(ground_truth_path)\n",
    "    \n",
    "    print(f\"Ground truth data: {len(ground_truth_df)} pairs\")\n",
    "    print(f\"Positive pairs: {ground_truth_df['match'].sum()}\")\n",
    "    print(f\"Negative pairs: {len(ground_truth_df) - ground_truth_df['match'].sum()}\")\n",
    "    \n",
    "    # If we have match pairs, we can compute precision, recall, and F1 score\n",
    "    if 'match_pairs_df' in locals():\n",
    "        # Create a set of match pairs from our results\n",
    "        predicted_pairs = set()\n",
    "        for _, row in match_pairs_df.iterrows():\n",
    "            pair = tuple(sorted([row['entity1'], row['entity2']]))\n",
    "            predicted_pairs.add(pair)\n",
    "        \n",
    "        # Create a set of match pairs from ground truth\n",
    "        true_pairs = set()\n",
    "        for _, row in ground_truth_df.iterrows():\n",
    "            if row['match']:\n",
    "                pair = tuple(sorted([row['left'], row['right']]))\n",
    "                true_pairs.add(pair)\n",
    "        \n",
    "        # Compute metrics\n",
    "        true_positives = len(predicted_pairs.intersection(true_pairs))\n",
    "        false_positives = len(predicted_pairs - true_pairs)\n",
    "        false_negatives = len(true_pairs - predicted_pairs)\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(\"\\n=== Entity Resolution Quality ===\")\n",
    "        print(f\"True Positives: {true_positives}\")\n",
    "        print(f\"False Positives: {false_positives}\")\n",
    "        print(f\"False Negatives: {false_negatives}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "else:\n",
    "    print(\"Ground truth data not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Analysis\n",
    "\n",
    "Analyze feature distributions and correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would require access to the feature vectors\n",
    "# As a placeholder, we can visualize the feature importance if available\n",
    "if 'feature_df' in locals():\n",
    "    # Plot feature importance as a pie chart for top 8 features\n",
    "    top_features = feature_df.head(8)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.pie(top_features['Importance'], labels=top_features['Feature'], autopct='%1.1f%%')\n",
    "    plt.title('Top 8 Features by Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Feature data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Weights Analysis\n",
    "\n",
    "Analyze the learned model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model weights if available\n",
    "from src.classification import Classifier\n",
    "\n",
    "# This requires loading the classifier state from a checkpoint\n",
    "# As a placeholder, we can visualize the feature importance from earlier\n",
    "if 'feature_importance' in locals():\n",
    "    print(\"Model weights analysis would go here, using feature importance as proxy\")\n",
    "else:\n",
    "    print(\"Model weights not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Runtime Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load logfile and extract timing information\n",
    "logfile_path = Path(config['monitoring']['logging']['file'])\n",
    "if logfile_path.exists():\n",
    "    # Simple parsing of log lines containing timing information\n",
    "    import re\n",
    "    \n",
    "    # Extract timing information\n",
    "    timing_pattern = r'(\\w+) completed in ([\\d\\.]+) seconds'\n",
    "    timings = {}\n",
    "    \n",
    "    with open(logfile_path, 'r') as f:\n",
    "        for line in f:\n",
    "            match = re.search(timing_pattern, line)\n",
    "            if match:\n",
    "                stage = match.group(1).lower()\n",
    "                time = float(match.group(2))\n",
    "                timings[stage] = time\n",
    "    \n",
    "    if timings:\n",
    "        # Display timing information\n",
    "        timing_df = pd.DataFrame({\n",
    "            'Stage': timings.keys(),\n",
    "            'Time (seconds)': timings.values()\n",
    "        }).sort_values('Time (seconds)', ascending=False)\n",
    "        \n",
    "        print(\"=== Runtime Performance ===\")\n",
    "        display(timing_df)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(data=timing_df, x='Time (seconds)', y='Stage')\n",
    "        plt.title('Pipeline Stage Runtime Performance')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No timing information found in logfile\")\n",
    "else:\n",
    "    print(\"Logfile not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analysis above, here are some key findings and recommendations:\n",
    "\n",
    "1. **Entity Resolution Quality**:\n",
    "   - Precision: [Value from analysis]\n",
    "   - Recall: [Value from analysis]\n",
    "   - F1 Score: [Value from analysis]\n",
    "   - Recommendation: [Based on metrics]\n",
    "\n",
    "2. **Feature Importance**:\n",
    "   - Top features: [List top features]\n",
    "   - Recommendation: [Based on feature analysis]\n",
    "\n",
    "3. **Clustering Performance**:\n",
    "   - Total clusters: [Value from analysis]\n",
    "   - Cluster size distribution: [Brief description]\n",
    "   - Recommendation: [Based on clustering analysis]\n",
    "\n",
    "4. **Runtime Performance**:\n",
    "   - Bottlenecks: [Identify slowest stages]\n",
    "   - Recommendation: [Based on timing analysis]\n",
    "\n",
    "5. **Overall Recommendations**:\n",
    "   - [General recommendations for improving the entity resolution pipeline]\n",
    "   - [Suggestions for potential optimizations]\n",
    "   - [Ideas for further exploration]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
